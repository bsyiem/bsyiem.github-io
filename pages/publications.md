---
layout: single
classes: wide
title: "Select Publications"
permalink: /publications/
header:
#  image: /assets/images/hero_img.JPG
  overlay_image: /assets/images/hero_img.JPG
  overlay_filter: 0.5

author_profile: true
---

[BIT 2024](https://doi.org/10.1080/0144929X.2024.2441963){: .btn .btn--info}   [PDF](/assets/pdfs/syiem2024systematic.pdf){: .btn .btn--info target="_blank"}  
**A Systematic Exploration of Collaborative Immersive Systems for Sense-making in STEM**{: style="font-size: larger"}  
***Brandon V. Syiem**, Selen Türkay*{: style="font-size: small"}

{% include figure popup=true image_path= "/assets/images/publications/syiem2024systematic.jpg" %}

Scientific sense-making in STEM fields is a complex, yet essential activity, that greatly benefits from collaborations. However, challenges associated with collaboration, such as the geographic separation of experts, access to specialised equipment, and meaningful data representation, often hinder this process. Solutions to collaborative challenges have been extensively explored in CSCW and HCI literature. Among such solutions, immersive systems offer novel data visualisations, interactions, and representations that can support collaborative sense-making in STEM fields. Recognising the increasing interest from HCI researchers on the intersection of collaboration and immersive systems, we conduct a systematic review to answer pertinent questions regarding the research landscape, the design and implementation of collaborative immersive systems for STEM sense-making. We find that current research leans towards synchronous collaborations, AR technology, and sense-making for learning in science domains. We further discuss prevalent trends and considerations observed in our findings, to inform future research directions.
{: .text-justify}  
---

[IJHCS 2024](https://doi.org/10.1016/j.ijhcs.2024.103324){: .btn .btn--info}   [PDF](/assets/pdfs/syiem2024addressing.pdf){: .btn .btn--info}  
**Addressing Attentional Issues in Augmented Reality with Adaptive Agents: Possibilities and Challenges**{: style="font-size: larger"}  
***Brandon Victor Syiem**, Ryan M. Kelly, Tilman Dingler, Jorge Goncalves, Eduardo Velloso*{: style="font-size: small"}

{% include video id="Rjt968MGxqI" provider="youtube" %}

Recent work on augmented reality (AR) has explored the use of adaptive agents to overcome attentional issues that negatively impact task performance. However, despite positive technical evaluations, adaptive agents have shown no significant improvements to user task performance in AR. Furthermore, previous works have primarily evaluated such agents using abstract tasks. In this paper, we develop an agent that observes user behaviour and performs appropriate actions to mitigate attentional issues in a realistic sense-making task in AR. We employ mixed methods to evaluate our agent in a between-subject experiment (N=60) to understand the agent’s effect on user task performance and behaviour. While we find no significant improvements in task performance, our analysis revealed that users’ preferences and trust in the agent affected their receptiveness of the agent’s recommendations. We discuss the pitfalls of autonomous agents and highlight the need to shift from designing better Human–AI interactions to better Human–AI collaborations.
{: .text-justify}  
---

[CHI 2024](https://doi.org/10.1145/3613904.3642015){: .btn .btn--info}   [PDF](/assets/pdfs/syiem2024augmented.pdf){: .btn .btn--info}  
**Augmented Reality at Zoo Exhibits: A Design Framework for Enhancing the Zoo Experience**{: style="font-size: larger"}  
***Brandon V. Syiem**, Sarah Webber, Ryan M. Kelly, Qiushi Zhou, Jorge Goncalves, Eduardo Velloso*{: style="font-size: small"}

{% include figure popup=true image_path= "/assets/images/publications/syiem2024augmented.jpg" %}

Augmented Reality (AR) offers unique opportunities for contributing to zoos’ objectives of public engagement and education about animal and conservation issues. However, the diversity of animal exhibits pose challenges in designing AR applications that are not encountered in more controlled environments, such as museums. To support the design of AR applications that meaningfully engage the public with zoo objectives, we first conducted two scoping reviews to interrogate previous work on AR and broader technology use at zoos. We then conducted a workshop with zoo representatives to understand the challenges and opportunities in using AR to achieve zoo objectives. Additionally, we conducted a field trip to a public zoo to identify exhibit characteristics that impacts AR application design. We synthesise the findings from these studies into a framework that enables the design of diverse AR experiences. We illustrate the utility of the framework by presenting two concepts for feasible AR applications.
{: .text-justify}  
---

[CHI 2024](https://doi.org/10.1145/3613904.3642814){: .btn .btn--info}   [PDF](/assets/pdfs/irlitti2024volumetric.pdf){: .btn .btn--info}  
**Volumetric Hybrid Workspaces: Interactions with Objects in Remote and Co-located Telepresence**{: style="font-size: larger"}  
*Andrew Irlitti, Mesut Latifoglu, Thuong Hoang, **Brandon Victor Syiem**, Frank Vetere*{: style="font-size: small"}

{% include figure popup=true image_path= "/assets/images/publications/irlitti2024volumetric.jpg" %}

Volumetric telepresence aims to create a shared space, allowing people in local and remote settings to collaborate seamlessly. Prior telepresence examples typically have asymmetrical designs, with volumetric capture in one location and objects in one format. In this paper, we present a volumetric telepresence mixed reality system that supports real-time, symmetrical, multi-user, partially distributed interactions, using objects in multiple formats, across multiple locations. We align two volumetric environments around a common spatial feature to create a shared workspace for remote and co-located people using objects in three formats: physical, virtual, and volumetric. We conducted a study with 18 participants over 6 sessions, evaluating how telepresence workspaces support spatial coordination and hybrid communication for co-located and remote users undertaking collaborative tasks. Our findings demonstrate the successful integration of remote spaces, effective use of proxemics and deixis to support negotiation, and strategies to manage interactivity in hybrid workspaces.
{: .text-justify}  
---

[IMWUT 2024](https://doi.org/10.1145/3631431){: .btn .btn--info}  [PDF](/assets/pdfs/zhou2024reflected.pdf){: .btn .btn--info}  
**Reflected Reality: Augmented Reality through the Mirror**{: style="font-size: larger"}  
*Qiushi Zhou, **Brandon Victor Syiem**, Beier Li, Jorge Goncalves, Eduardo Velloso*{: style="font-size: small"}

{% include video id="_jFqzPX5Fvs" provider="youtube" %}

We propose Reflected Reality: a new dimension for augmented reality that expands the augmented physical space into mirror reflections. By synchronously tracking the physical space in front of the mirror and the reflection behind it using an AR headset and an optional smart mirror component, reflected reality enables novel AR interactions that allow users to use their physical and reflected bodies to find and interact with virtual objects. We propose a design space for AR interaction with mirror reflections, and instantiate it using a prototype system featuring a HoloLens 2 and a smart mirror. We explore the design space along the following dimensions: the user's perspective of input, the spatial frame of reference, and the direction of the mirror space relative to the physical space. Using our prototype, we visualise a use case scenario that traverses the design space to demonstrate its interaction affordances in a practical context. To understand how users perceive the intuitiveness and ease of reflected reality interaction, we conducted an exploratory and a formal user evaluation studies to characterise user performance of AR interaction tasks in reflected reality. We discuss the unique interaction affordances that reflected reality offers, and outline possibilities of its future applications.
{: .text-justify}  
---

[Top](#){: .btn .btn--inverse}
{: .text-right}

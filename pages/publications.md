---
layout: single
classes: wide
title: "Select Publications"
permalink: /publications/
header:
#  image: /assets/images/hero_img.JPG
  overlay_image: /assets/images/hero_img.JPG
  overlay_filter: 0.5

author_profile: true
---

[BIT 2024](https://doi.org/10.1080/0144929X.2024.2441963){: .btn .btn--info target="_blank"}   [PDF](/assets/pdfs/syiem2024systematic.pdf){: .btn .btn--info target="_blank"}  
**A Systematic Exploration of Collaborative Immersive Systems for Sense-making in STEM**{: style="font-size: larger"}  
***Brandon V. Syiem**, Selen Türkay*{: style="font-size: small"}

{% include figure popup=true image_path= "/assets/images/publications/syiem2024systematic.jpg" %}

Scientific sense-making in STEM fields is a complex, yet essential activity, that greatly benefits from collaborations. However, challenges associated with collaboration, such as the geographic separation of experts, access to specialised equipment, and meaningful data representation, often hinder this process. Solutions to collaborative challenges have been extensively explored in CSCW and HCI literature. Among such solutions, immersive systems offer novel data visualisations, interactions, and representations that can support collaborative sense-making in STEM fields. Recognising the increasing interest from HCI researchers on the intersection of collaboration and immersive systems, we conduct a systematic review to answer pertinent questions regarding the research landscape, the design and implementation of collaborative immersive systems for STEM sense-making. We find that current research leans towards synchronous collaborations, AR technology, and sense-making for learning in science domains. We further discuss prevalent trends and considerations observed in our findings, to inform future research directions.
{: .text-justify}  
---

[IJHCS 2024](https://doi.org/10.1016/j.ijhcs.2024.103324){: .btn .btn--info target="_blank"}   [PDF](/assets/pdfs/syiem2024addressing.pdf){: .btn .btn--info target="_blank"}  
**Addressing Attentional Issues in Augmented Reality with Adaptive Agents: Possibilities and Challenges**{: style="font-size: larger"}  
***Brandon Victor Syiem**, Ryan M. Kelly, Tilman Dingler, Jorge Goncalves, Eduardo Velloso*{: style="font-size: small"}

{% include video id="Rjt968MGxqI" provider="youtube" %}

Recent work on augmented reality (AR) has explored the use of adaptive agents to overcome attentional issues that negatively impact task performance. However, despite positive technical evaluations, adaptive agents have shown no significant improvements to user task performance in AR. Furthermore, previous works have primarily evaluated such agents using abstract tasks. In this paper, we develop an agent that observes user behaviour and performs appropriate actions to mitigate attentional issues in a realistic sense-making task in AR. We employ mixed methods to evaluate our agent in a between-subject experiment (N=60) to understand the agent’s effect on user task performance and behaviour. While we find no significant improvements in task performance, our analysis revealed that users’ preferences and trust in the agent affected their receptiveness of the agent’s recommendations. We discuss the pitfalls of autonomous agents and highlight the need to shift from designing better Human–AI interactions to better Human–AI collaborations.
{: .text-justify}  
---

[CHI 2024](https://doi.org/10.1145/3613904.3642015){: .btn .btn--info target="_blank"}   [PDF](/assets/pdfs/syiem2024augmented.pdf){: .btn .btn--info target="_blank"}  
**Augmented Reality at Zoo Exhibits: A Design Framework for Enhancing the Zoo Experience**{: style="font-size: larger"}  
***Brandon V. Syiem**, Sarah Webber, Ryan M. Kelly, Qiushi Zhou, Jorge Goncalves, Eduardo Velloso*{: style="font-size: small"}

{% include figure popup=true image_path= "/assets/images/publications/syiem2024augmented.jpg" %}

Augmented Reality (AR) offers unique opportunities for contributing to zoos’ objectives of public engagement and education about animal and conservation issues. However, the diversity of animal exhibits pose challenges in designing AR applications that are not encountered in more controlled environments, such as museums. To support the design of AR applications that meaningfully engage the public with zoo objectives, we first conducted two scoping reviews to interrogate previous work on AR and broader technology use at zoos. We then conducted a workshop with zoo representatives to understand the challenges and opportunities in using AR to achieve zoo objectives. Additionally, we conducted a field trip to a public zoo to identify exhibit characteristics that impacts AR application design. We synthesise the findings from these studies into a framework that enables the design of diverse AR experiences. We illustrate the utility of the framework by presenting two concepts for feasible AR applications.
{: .text-justify}  
---

[CHI 2024](https://doi.org/10.1145/3613904.3642814){: .btn .btn--info target="_blank"}   [PDF](/assets/pdfs/irlitti2024volumetric.pdf){: .btn .btn--info target="_blank"}  
**Volumetric Hybrid Workspaces: Interactions with Objects in Remote and Co-located Telepresence**{: style="font-size: larger"}  
*Andrew Irlitti, Mesut Latifoglu, Thuong Hoang, **Brandon Victor Syiem**, Frank Vetere*{: style="font-size: small"}

{% include figure popup=true image_path= "/assets/images/publications/irlitti2024volumetric.jpg" %}

Volumetric telepresence aims to create a shared space, allowing people in local and remote settings to collaborate seamlessly. Prior telepresence examples typically have asymmetrical designs, with volumetric capture in one location and objects in one format. In this paper, we present a volumetric telepresence mixed reality system that supports real-time, symmetrical, multi-user, partially distributed interactions, using objects in multiple formats, across multiple locations. We align two volumetric environments around a common spatial feature to create a shared workspace for remote and co-located people using objects in three formats: physical, virtual, and volumetric. We conducted a study with 18 participants over 6 sessions, evaluating how telepresence workspaces support spatial coordination and hybrid communication for co-located and remote users undertaking collaborative tasks. Our findings demonstrate the successful integration of remote spaces, effective use of proxemics and deixis to support negotiation, and strategies to manage interactivity in hybrid workspaces.
{: .text-justify}  
---

[IMWUT 2024](https://doi.org/10.1145/3631431){: .btn .btn--info target="_blank"}  [PDF](/assets/pdfs/zhou2024reflected.pdf){: .btn .btn--info target="_blank"}  
**Reflected Reality: Augmented Reality through the Mirror**{: style="font-size: larger"}  
*Qiushi Zhou, **Brandon Victor Syiem**, Beier Li, Jorge Goncalves, Eduardo Velloso*{: style="font-size: small"}

{% include video id="_jFqzPX5Fvs" provider="youtube" %}

We propose Reflected Reality: a new dimension for augmented reality that expands the augmented physical space into mirror reflections. By synchronously tracking the physical space in front of the mirror and the reflection behind it using an AR headset and an optional smart mirror component, reflected reality enables novel AR interactions that allow users to use their physical and reflected bodies to find and interact with virtual objects. We propose a design space for AR interaction with mirror reflections, and instantiate it using a prototype system featuring a HoloLens 2 and a smart mirror. We explore the design space along the following dimensions: the user's perspective of input, the spatial frame of reference, and the direction of the mirror space relative to the physical space. Using our prototype, we visualise a use case scenario that traverses the design space to demonstrate its interaction affordances in a practical context. To understand how users perceive the intuitiveness and ease of reflected reality interaction, we conducted an exploratory and a formal user evaluation studies to characterise user performance of AR interaction tasks in reflected reality. We discuss the unique interaction affordances that reflected reality offers, and outline possibilities of its future applications.
{: .text-justify}  
---

[CEXR 2023](https://doi.org/10.1016/j.cexr.2023.100037){: .btn .btn--info target="_blank"}   [PDF](/assets/pdfs/khorasani2023hands.pdf){: .btn .btn--info target="_blank"}  
**Hands-on or Hands-off: Deciphering the Impact of Interactivity on Embodied Learning in VR**{: style="font-size: larger"}  
*Sara Khorasani, **Brandon Victor Syiem**, Sadia Nawaz, Jarrod Knibbe, Eduardo Velloso*{: style="font-size: small"}

{% include figure popup=true image_path= "/assets/images/publications//khorasani2023hands.jpg" %}

Studies suggest that Sense of Embodiment (SoE) enabled by VR promotes embodied and active learning. However, it is unclear what features of VR learning environments tap into the concept of embodied learning. For example, interaction techniques, movement and purely observational scenarios in VR can all play a role in facilitating embodied learning. To understand how these mechanisms impact learning, we conducted 2 studies with a total of 64 participants who had no prior experience in the training task. Participants were taught how to use a table saw in 4 conditions and were tested on their task performance in a fully interactive VR assessment. The conditions were analyzed in pairs; 2 conditions with different interaction techniques, 2 conditions with differing ability to move and a cross-study analysis comparing conditions with purely observational learning to interactive learning. We used a mixed methods approach; Analysis of Variance (ANOVA), pairwise comparison of the learning outcomes in each condition as well as thematic analysis of the interview results. We found that some types of “hands-on” interactions can have a detrimental impact on learning and that observational learning can be as impactful as a fully interactive experience. Based on participant interviews, we explored how these mechanisms of the learning environment can impact participants’ learning ability.
{: .text-justify}  
---

[IEEE ACCESS 2023](https://doi.org/10.1109/ACCESS.2023.3297882){: .btn .btn--info target="_blank"}   [PDF](/assets/pdfs/zehra2023evaluation.pdf){: .btn .btn--info target="_blank"}  
**Evaluation of Optimal Stimuli for SSVEP-Based Augmented Reality Brain-Computer Interfaces**{: style="font-size: larger"}  
*Syeda R. Zehra, Jing Mu, **Brandon V. Syiem**, Anthony N. Burkitt, David B. Grayden*{: style="font-size: small"}

{% include figure popup=true image_path= "/assets/images/publications//zehra2023evaluation.jpg" %}

Steady-State Visually Evoked Potentials (SSVEPs) serve as one of the most robust Brain-Computer Interface (BCI) paradigms. Being an exogenous brain response, the properties of elicited SSVEPs are directly related to the properties of the visual stimuli. However, studies on integrating BCI and Augmented Reality (AR), aimed at realising mobile BCI systems, have mainly focused on applications of BCIs and performance comparison with screen-based BCIs. Little work has been done to study the effects of stimulus parameters on BCI performance when stimuli are presented with an AR headset. Here, we compare AR-based SSVEP with 3D and 2D stimuli using three different stimulation strategies: flickering, grow-shrink, and both. Participant feedback on level of fatigue and their subjective preference of stimuli were also collected. Our results did not show significant differences in classification accuracies between the 2D and 3D stimuli. However, for most of the participants, classification accuracy with flickering stimuli was above their average performance and stimuli that changed only in size were below average. The participants were divided in terms of which type of stimulus they felt was the most comfortable.
{: .text-justify}  

[Top](#){: .btn .btn--inverse}
{: .text-right}
